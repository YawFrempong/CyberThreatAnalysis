{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depend Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import ast\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Annotated Data from Cybertweet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./twitter_data.csv')\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Input Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert column type values from (string): '['botnet']' into (list): ['botnet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = []\n",
    "\n",
    "for type_as_str in df['type']:\n",
    "    type_as_list = ast.literal_eval(type_as_str)\n",
    "    \n",
    "    #standardize types to lower case: 'vulnerability' and 'Vulnerability' should be the same type\n",
    "    for i in range(len(type_as_list)):\n",
    "        type_as_list[i] = type_as_list[i].lower()\n",
    "    \n",
    "    new_data.append(type_as_list)\n",
    "    \n",
    "df['type'] = new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = df['type']\n",
    "multilabel = MultiLabelBinarizer()\n",
    "types_encoded = multilabel.fit_transform(df['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(types_encoded, columns=multilabel.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Input Text into a Sparse Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I could use the max_feature argument -> limits dictionary representation -> faster training. I won't use it here.\n",
    "tfidf = TfidfVectorizer(analyzer='word')\n",
    "text_vectorized = tfidf.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorized.shape, types_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Testing and Training Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_Train, text_Test, type_Train, type_Test = train_test_split(text_vectorized, types_encoded, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance Measurement Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def j_score(y_true, y_pred):\n",
    "    jaccard = np.minimum(y_true, y_pred).sum(axis = 1) / np.maximum(y_true, y_pred).sum(axis = 1)\n",
    "    return jaccard.mean()*100\n",
    "\n",
    "def print_score(y_pred, clf):\n",
    "    print('clf: ', clf.__class__.__name__)\n",
    "    print('Jacard score: {}'.format(j_score(type_Test, y_pred)))\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier()\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "svc = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare performance\n",
    "for classifier in [sgd, lr, svc]:\n",
    "    clf = OneVsRestClassifier(classifier)\n",
    "    clf.fit(text_Train, type_Train)\n",
    "    pred = clf.predict(text_Test)\n",
    "    print_score(pred, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input = input('Enter Some Text:')\n",
    "    if user_input == 'stop':\n",
    "        break\n",
    "        \n",
    "    #transform input into sparse matrix\n",
    "    input_transform = tfidf.transform([user_input])\n",
    "    #decode prediction into text classifications\n",
    "    print('Classification:', multilabel.inverse_transform(clf.predict(input_transform)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with CVE Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv('./cve_data_description_only.csv')\n",
    "df_2.dropna(inplace=True)\n",
    "\n",
    "exclude_str = '**'\n",
    "cve_arr = []\n",
    "\n",
    "for cve in df_2['Description']:\n",
    "    if exclude_str not in cve:\n",
    "        cve_arr.append(cve)\n",
    "        \n",
    "#remove duplicates\n",
    "cve_arr = list(set(cve_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "counter = 0\n",
    "\n",
    "for cve in cve_arr:\n",
    "    input_transform = tfidf.transform([cve])\n",
    "    prediction = multilabel.inverse_transform(clf.predict(input_transform))\n",
    "    prediction_str = str(prediction)\n",
    "    results.append(prediction_str)\n",
    "    \n",
    "    if counter < 500:\n",
    "        print('Input:', cve)\n",
    "        print('Classification:', prediction_str)\n",
    "        print('-----------------')\n",
    "        \n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['Description', 'Classification']\n",
    "output_data = []\n",
    "for a,b in zip(cve_arr, results):\n",
    "    output_data.append([a,b])\n",
    "\n",
    "with open('cve_classified.csv', 'w') as f: \n",
    "      \n",
    "    write = csv.writer(f) \n",
    "    write.writerow(fields) \n",
    "    write.writerows(output_data)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
