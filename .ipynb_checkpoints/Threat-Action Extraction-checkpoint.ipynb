{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependent Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import textacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Load Data and Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./cve_data_description_only.csv')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "cve_arr = []\n",
    "\n",
    "for cve in df['Description']:\n",
    "    if '**' not in cve:\n",
    "        cve_arr.append(cve)\n",
    "\n",
    "#remove duplicates\n",
    "cve_arr = list(set(cve_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Using Regular Expressions to Replace IP Addresses, URLs, exe files, and CVE Report IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_regex = r'[0-9]+(?:\\.[0-9]+){3}'\n",
    "url_regex = r'(?P<url>https?://[^\\s]+)'\n",
    "exe_regex = r'\\b\\S*\\.exe\\b'\n",
    "report_regex = r'CVE-\\d{4}-\\d{4,7}'\n",
    "\n",
    "temp = cve_arr[:]\n",
    "cve_arr_clean = []\n",
    "ip_slot_map = []\n",
    "\n",
    "\n",
    "for i in range(len(temp)):\n",
    "    cve = temp[i]\n",
    "    ip_arr = re.findall(ip_regex, cve)\n",
    "    \n",
    "    if len(ip_arr) > 0:\n",
    "        ip_slot_map.append({'index': i, 'ip_data': ip_arr})\n",
    "    \n",
    "    for ip in ip_arr:\n",
    "        cve = cve.replace(ip,'IP_ADDRESS_STRING')\n",
    "    cve_arr_clean.append(cve)\n",
    "\n",
    "#subsitute URLs\n",
    "temp = cve_arr_clean[:]\n",
    "cve_arr_clean = []\n",
    "url_slot_map = []\n",
    "\n",
    "for i in range(len(temp)):\n",
    "    cve = temp[i]\n",
    "    url_arr = re.findall(url_regex, cve)\n",
    "    if len(url_arr) > 0:\n",
    "        url_slot_map.append({'index': i, 'url_data': url_arr})\n",
    "    \n",
    "    for url in url_arr:\n",
    "        cve = cve.replace(url,'URL_STRING')\n",
    "    cve_arr_clean.append(cve)\n",
    "    \n",
    "#subsitute executable files\n",
    "temp = cve_arr_clean[:]\n",
    "cve_arr_clean = []\n",
    "exe_slot_map = []\n",
    "\n",
    "for i in range(len(temp)):\n",
    "    cve = temp[i]\n",
    "    exe_arr = re.findall(exe_regex, cve)\n",
    "    if len(exe_arr) > 0:\n",
    "        exe_slot_map.append({'index': i, 'exe_data': exe_arr})\n",
    "    \n",
    "    for exe in exe_arr:\n",
    "        cve = cve.replace(exe,'EXE_STRING')\n",
    "    cve_arr_clean.append(cve)\n",
    "    \n",
    "#subsitute CVE report labels\n",
    "temp = cve_arr_clean[:]\n",
    "cve_arr_clean = []\n",
    "report_slot_map = []\n",
    "\n",
    "for i in range(len(temp)):\n",
    "    cve = temp[i]\n",
    "    report_arr = re.findall(report_regex, cve)\n",
    "    if len(report_arr) > 0:\n",
    "        report_slot_map.append({'index': i, 'report_data': report_arr})\n",
    "    \n",
    "    for report in report_arr:\n",
    "        cve = cve.replace(report,'REPORT_STRING')\n",
    "    cve_arr_clean.append(cve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Remove Stop Words, Numbers, and Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I found more complete results without the additional preprocessing\n",
    "'''\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "temp = cve_arr_clean[:]\n",
    "cve_arr_clean = []\n",
    "\n",
    "for cve in temp:\n",
    "    word_tokens = word_tokenize(cve) \n",
    "    filtered_sentence = [w for w in word_tokens if (not w in stop_words) and (w.isalpha())] \n",
    "    str_val = ' '\n",
    "    str_val = str_val.join(filtered_sentence)\n",
    "    cve_arr_clean.append(str_val)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subject-Verb-Object Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_canindates(SVO_arr):\n",
    "    canindates = []\n",
    "\n",
    "    for tup in SVO_arr:\n",
    "\n",
    "        #convert tuple to list of strings\n",
    "        tup_as_list = []\n",
    "        for val in tup:\n",
    "            tup_as_list.append(str(val))\n",
    "\n",
    "        #convert list of strings to single string\n",
    "        list_as_str = \" \"\n",
    "        list_as_str = list_as_str.join(tup_as_list)\n",
    "\n",
    "        #create list of canindate threat actions for each cve\n",
    "        canindates.append(list_as_str)\n",
    "\n",
    "    return canindates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Couldn't get Stanford Dependency Parser to work so I choose spaCy to extract the SVOs(Subject-Verb-Object)\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of CVEs\n",
    "#Each CVEs has a list of SVOs strings for each sentence -> ['Sentence 1 SVO', 'Sentence 2 SVO', 'Sentence 3 SVO',....]\n",
    "SVOs_for_each_CVE =  []\n",
    "\n",
    "for clean_cve in cve_arr_clean:\n",
    "    text = nlp(clean_cve)\n",
    "    text_ext = textacy.extract.subject_verb_object_triples(text)\n",
    "    SVO_arr = list(text_ext)\n",
    "    SVOs_for_each_CVE.append(generate_canindates(SVO_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Similarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score(scorer,input_arr):\n",
    "    score_arr = scorer.get_scores(input_arr).tolist()\n",
    "    max_score = max(score_arr)\n",
    "    max_idx = score_arr.index(max_score)\n",
    "    max_val = threat_ontology[max_idx]\n",
    "    return max(score_arr), max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use sliding window method with a size of 3\n",
    "#Consider 3 sentences at a time\n",
    "#Decide whether to group the sentences in the window or keep them seperate for each CVE\n",
    "\n",
    "#Bag of Words representation of our cyber threat ontology \n",
    "threat_ontology = [\n",
    "    ['phishing', 'spear', 'whale'],\n",
    "    ['malware', 'ransomware', 'macro','virus','file', 'infectors', 'system', 'boot-record', 'polymorphic', 'sleath', 'trojan', 'horses', 'logic', 'bomb', 'worms', 'droppers', 'adware' , 'spyware'],\n",
    "    ['web', 'cross', 'site', 'scripting'],\n",
    "    ['ddos', 'TCP','SYN', 'flood', 'teardrop', 'smurf', 'ping', 'death', 'botnets'],\n",
    "    ['mitm', 'session', 'hijacking', 'ip', 'spoofing', 'replay'],\n",
    "    ['password', 'brute', 'force', 'dictionary'],\n",
    "    ['eavesdropping', 'passive', 'active']\n",
    "]\n",
    "\n",
    "#train the ranking function on the bag of word representation of our cyber threat ontology\n",
    "bm25 = BM25Okapi(threat_ontology)\n",
    "\n",
    "#final output array\n",
    "results = []\n",
    "\n",
    "#sliding window\n",
    "window_size = 3\n",
    "for x in range(len(SVOs_for_each_CVE)):\n",
    "    SVOs = SVOs_for_each_CVE[x]\n",
    "    \n",
    "    #iterate of the SVOs for each sentence in a single CVE\n",
    "    #only consider grouping sentences of CVE that have at least 3 sentences\n",
    "    if len(SVOs) >= window_size:\n",
    "        i = 0\n",
    "        last_idx = window_size - 1\n",
    "        long_cve_result = []\n",
    "        \n",
    "        while i < (len(SVOs) - last_idx):\n",
    "            current_window = SVOs[i:(i+window_size)]\n",
    "            tokenized_current_window = [doc.split(\" \") for doc in current_window]\n",
    "            single_canindate = tokenized_current_window[0]\n",
    "            multiple_canindates = tokenized_current_window[0] + tokenized_current_window[1] + tokenized_current_window[2]\n",
    "            single_score, single_val = similarity_score(bm25, single_canindate)\n",
    "            multi_score, multi_val = similarity_score(bm25, multiple_canindates)\n",
    "            \n",
    "            if single_score == 0 and multi_score == 0:\n",
    "                long_cve_result.append([])\n",
    "                i += 1\n",
    "            else:\n",
    "                if multi_score > single_score:\n",
    "                    long_cve_result.append(multi_val)\n",
    "                    i += 3\n",
    "                else:\n",
    "                    long_cve_result.append(single_val)\n",
    "                    i += 1\n",
    "        \n",
    "        results.append(long_cve_result)\n",
    "    elif len(SVOs) == 2:\n",
    "        current_window = SVOs\n",
    "        tokenized_current_window = [doc.split(\" \") for doc in current_window]\n",
    "        single_canindate = tokenized_current_window[0]\n",
    "        multiple_canindates = tokenized_current_window[0] + tokenized_current_window[1]\n",
    "        single_score, single_val = similarity_score(bm25, single_canindate)\n",
    "        multi_score, multi_val = similarity_score(bm25, multiple_canindates)\n",
    "\n",
    "        if single_score == 0 and multi_score == 0:\n",
    "            results.append([])\n",
    "        else:\n",
    "            if multi_score > single_score:\n",
    "                results.append(multi_val)\n",
    "            else:\n",
    "                results.append(single_val)\n",
    "    elif len(SVOs) == 1:\n",
    "        current_window = SVOs\n",
    "        tokenized_current_window = [doc.split(\" \") for doc in current_window]\n",
    "        single_canindate = tokenized_current_window[0]\n",
    "        single_score, single_val = similarity_score(bm25, single_canindate)\n",
    "        if single_score == 0:\n",
    "            results.append([])\n",
    "        else:\n",
    "            results.append(single_val)\n",
    "    else:\n",
    "        results.append([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "['web', 'cross', 'site', 'scripting'] \n",
      "\n",
      "['web', 'cross', 'site', 'scripting'] \n",
      "\n",
      "[] \n",
      "\n",
      "['password', 'brute', 'force', 'dictionary'] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[[], []] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[[]] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[[], []] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "['web', 'cross', 'site', 'scripting'] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "['web', 'cross', 'site', 'scripting'] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[['malware', 'ransomware', 'macro', 'virus', 'file', 'infectors', 'system', 'boot-record', 'polymorphic', 'sleath', 'trojan', 'horses', 'logic', 'bomb', 'worms', 'droppers', 'adware', 'spyware'], []] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[[], []] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[[], []] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[[]] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "['web', 'cross', 'site', 'scripting'] \n",
      "\n",
      "[] \n",
      "\n",
      "[['malware', 'ransomware', 'macro', 'virus', 'file', 'infectors', 'system', 'boot-record', 'polymorphic', 'sleath', 'trojan', 'horses', 'logic', 'bomb', 'worms', 'droppers', 'adware', 'spyware']] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[[]] \n",
      "\n",
      "[[], []] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "['web', 'cross', 'site', 'scripting'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fields = ['Description', 'Ontology']\n",
    "output_data = []\n",
    "for a,b in zip(cve_arr, results):\n",
    "    output_data.append([a,str(b)])\n",
    "\n",
    "with open('cve_ontology.csv', 'w') as f: \n",
    "      \n",
    "    write = csv.writer(f) \n",
    "    write.writerow(fields) \n",
    "    write.writerows(output_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
